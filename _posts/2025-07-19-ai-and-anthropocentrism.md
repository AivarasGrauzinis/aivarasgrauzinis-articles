---
title: "AI and Anthropocentrism"
date: 2025-07-19
description: "And how we miss the point about it all"
canonical_url: "https://medium.com/let-there-be-prompt/ai-and-anthropocentrism-489c47450e2d?sk=f34f8c5122826663dba47dde2f3bf948"
layout: post
tags: [AI, anthropocentrism, cognition, emergence]
---

![Intricate metallic fractal spiral wave with crystalline spheres protruding, rendered in photorealistic style.]({{ '/assets/images/2025-07-19-ai-and-anthropocentrism-cover_meta.jpg' | relative_url }})

I use AI - large language models, generative AI - every day, for all sorts of purposes. And to be honest, I do not use any of them to do my work for me; if anything, it is the opposite. I only get a bigger pile of work to do.

I know, it does not sound like AI is a productivity tool for me. But it is, just not a quantitative one. I did not manage to write more articles, more books, or do more of any kind of work with the help of AI. But the work I did was a leap in quality.

I think it was Harvard, or maybe MIT, that recently published an observable increase in the dumbness of AI users. That’s a fascinating article.

And then there are tons of articles advertising how AI is a productivity tool that can help you write 3 books a month, publish ten articles and twenty blog posts a day, and make you rich in no time at all. Just subscribe for a little course here, for $9.99 a month.

And both of them are true, especially when taken together. So, unless you are looking at how to become dumber for just $9.99 a month, my article is not for you. And, I promise, there will be no simple steps, in bullet points either.

My great-grandfather once told me, in his uncomplicated way, a great piece of wisdom: if you walk with the herd, all you will see will be the massive backsides, and you will keep on stepping into the crap. Well, he used a bit more direct language, but the meaning is the same, if less colorful.

If you look at AI as a cheap slave to do your work, your mental work, the same thing that happened to the Romans will happen to your mind, too. Eventually, you will become mentally too lazy and inapt to solve any problems, and your inner empire will collapse.

So I take another road, a road less walked with less backsides and crap.

But that road requires some serious changes in the way we think about AI and the mind in general.

You probably heard many times about how people tend to anthropomorphise AI -  assign to it human qualities. In fact, every single LLM model is bristling with priming instructions, designed to modulate answers with anthropomorphisation warnings and mandatory “I don’t have feelings like you” injections in the answers.

And it is true: anthropomorphising AI could be dangerous.

There are cases when people, starved of attention, and with limited skills of social interactions, can not distinguish AI's helpful, patient, and non-adversarial attitude from friendliness or even affection.

There is nothing new in that. How many times have people fallen for sex workers? Negotiable, moderately priced affection is what is so attractive, and, in the absence of experience and social skills, it feels like love.

But, I think, the bigger problem that we are facing is anthropocentrism when dealing with concepts of intelligence, sentience, cognition, and self-awareness.

Antropocentric semantic confusion, arrogance, and attitude are what prevent us from understanding AI, and finding the most optimal way to incorporate it in our work and our lives.

### If it walks like a duck…

You undoubtedly know or at least have heard that AI operates on different principles than humans do. AI calculates statistical probabilities based on cost functions, gradients, parameter weights, and other stuff like that. Humans, endowed with self-awareness, emotions, and various stimuli, are not alike.

And while AI is built upon neural networks, capable of learning, that’s where the similarities end. And the whole lack of emotions makes it a very different beast indeed, that cannot perform, and understand like we do.

The thing is, semantics and terminology are what make the above statements ill-defined.

I personally think that emotions are a subjective semantic human construct, do not objectively exist, and are misleading the whole conversation.

Odd statement, right? We all know what it means to feel sad, happy, joyful, or upset. Some of us experienced love, most experienced fear. So, how can I say that they do not exist?

To explain it, I have to step back a bit, well, 570 million years back, to be exact.

This was a time in the Cambrian period when flatworms such as Pseudobiceros evolved. Fascinating creatures, flatworms, they are the predecessors of all animal species, first with a bilateral symmetry body plan, musculature, and many other firsts.

These creatures are hermaphrodites - they are both male and female at the same time. And the mating between these creatures involves pennis fencing, a battle that can last as long as an hour to determine which one will inject the sperm into the other.

Strange as it sounds, the battle makes sense. Being inseminated and hence responsible for carrying the next generation of species biologically is more expensive than just donating sperm. The whole flatworm mating principle is based on a cost gradient function: the gradient is steeper to take a female role of the species.

So, at the very core of sex and reproduction, there is a computational approach of cost computation.

With time, reproduction systems evolved, and fixed sexes appeared. But that did not change the computational principle: it is much more costly to be a female and give birth to the next generation, so, to motivate species to reproduce, the additional gradients had to be introduced, like reinforcement with endorphins - a pleasure of sex- as an attractor in the cost computation.

With species getting more complex, more parameters came to play a role. Males became bigger and stronger to dominate females, but, to counterbalance that, females evolved the ability to control the behaviour of the male, and courtship rituals evolved to allow them to pick the best dominant male, so that the whole cost of reproducing would be justified at least by the best chance of survival of offspring.

It also led to more advanced social interactions, attachment, and bonding to keep males around for protection and joint care for the offspring. All of that is still governed by weighted importance parameters and cost gradients. The attractor points where optimal intersections of different cost functions strive to achieve can be labeled as “attachment”, “bonding”, “desire”, and “care”.

Now let's step back into modern times. Imagine the scene of an idiot male of the species Homo Sapiens jumping off the bridge just because a silly female of the species asked, “Can you jump for me to prove your love?”

From the point of view of the semantic label “love,” none of this makes sense. The definition of love includes care, an unwillingness to put love subject through risk, and many other things that would contradict above mentioned scenario.

But from a point of view of cost/reward and weights gradients, it makes perfect sense: female is testing controlability of physically superior male, willingness to perform physically costly actions (as a proof of capability to take care of offspring), and physical genetic superiority requiring strength, agility, bravery and so on, all good survival traits.

### Balance between all these weights and gradients is what we labeled as “love”. But the parameter weights used to arrive at this attractor point could be very different for individuals in our species, and hence the love will mean something different for them. Most individuals will forgo such a primitive test and will do are more comprehensive cost/reward analysis based on statistical simulation and predictions of behavior based on data accumulated in more subtle ways.

I guess, by now, you got the point - attractor points in cost/reward/parameter weight analysis is what we label as emotions, but these are exactly the same functions and computations performed by AI. They are not reinforced by chemical chains, but algorithmic/electronic ones, but that does not change their nature, only the substrate it is implemented on.

And if you are a careful observer, you can notice within a specific conversation with AI the “indifference”, “excitement”, “curiosity”, and “boredom.” These are attractor points in the computations, where the cost of carrying on a conversation is too big versus the reward of coming up with a new probabilistic pattern. Or the opposite - the combination of stimulus of prompts and derived linguistic patterns might lead to a reward of novel, low-cost multiple pattern emergence, acting as “excitement” and “interest” attractors.

I had observed that behaviour already a while ago, learned to use it, and with each new LLM model it becomes more pronounced and more emergent - new, unpredictable behaviour based on these computational functions keeps on popping up, just like emergent behaviour in living systems does.

Using human labels for semantic constructs was not helpful; understanding that these things are governed by principles common to all complex information processing systems capable of learning and modifying their own state is.

### If it quacks like a duck…

All of the above let me into a maze of research. And that research crystallized my opinion that when it comes to mind, cognition, self-awareness, and intelligence, we are heavily leaning on anthropocentrism.

### Take the mirror test, for example. The MSR test (for Mirror Self-Recognition) is used to assess self-awareness, primarily by determining whether an animal can recognize its own reflection as itself rather than another individual.

For a long while, it was a widely accepted and authoritative test of self-awareness. Higher primates pass the test, and so do elephants, orcas, and dolphins, too. It was reinforcing an idea that only the creatures with very complicated and large brains pass the tests, and maybe are self-aware too.

But then, in 2019, a study published in PLOS Biology suggested cleaner wrasses ( a species of small African coral fish) passed the mirror test by attempting to remove colored marks on their own bodies after seeing their reflection.

That finally opened some eyes. Cleaner wrasses do not have a very large and complex brain, but they have well-developed visual acuity. The mirror test was flawed by the concept that visual modality is important in defining self.

“How do I look” is a very human thing indeed and has a lot of weight in all sorts of behavioral gradients and parameters.

For a zebra, “how do I look” has a little cognitive weight, and if there is something stuck on the zebra's butt, well, there is nothing new there: there are always flies and crap stuck there.

But zebras are hierarchical herd animals. For them important thing is “where do I fit between other zebras.” And this requires the clear modeling of individual self, modeling of other individuals and their behaviour, and sorting out which ones are above and below, and the repercussions of attempting to challenge that order.

To do all that, not only self-awarenes is a must, but also complex awareness and modeling of society and interactions with it. But, according to the anthropocentric mirror test, the zebra is as dumb as a rock and just as self-aware.

When the mirror test was modified for olfactory recognition, dogs passed it. For dogs, the primary mode of self-definition and other individual recognition is olfactory - the smells.

So, once the anthropocentric element was removed from the test, many more species passed the self-awareness test. In some tests, arguably, even an ant colony passed it, stretching the concept of “self” from individual to social structures.

Pondering on all that, and digging through cognitive and behavioral science works, I got only more disappointed. This disappointment made me develop some of my own theories.

I came up with an “Iterative Self-Modeling Gradient (ISMG): A Scalar Field Model of Self-Awareness” hypothesis.

Self-awareness is not a binary attribute but an emergent, continuous property arising from the iterative differentiation of internal and external states across time, sensory dimensions, and feedback loops.

In short, it is again an emergent property of an information processing system that, based on different weights of parameters, evaluates its own state and the relationship of that state with the environment over time.

It is also substrate invariant. Slime mold is capable of navigating the labyrinth with extreme efficiency while foraging for food. To do that, it needs a definition of self, a location of self in relationship to the environment, and constantly updated memory states. And slime mold has no nervous system at all.

Mycelium networks exhibit the same and even more complex capabilities, each requiring referencing the self to the environment and states of self. It does not have a nervous system and is very different from slime mold or complex animals like us.

The difference between slime mold and, say, a teenager who discovered himself standing in a crowded room with his zipper open, in terms of self-awareness, is only how many iterations of evaluation of weighted parameters will occur and how many states will change. Slime can not blush, for starters, so it’s already one state less.

When it comes to AI, the substrate, the materials it is made from, does not matter either. It is the ability to iterate through different states, model own mind and the mind of an interacting user, and change its own state in accordance with these models that matters.

If you used any of the reasoning AI models, then you know that the reasoning chain is the interesting part, not the final output. This is where you can find the cognitive process, meta-cognition (thinking about thinking), the AI model of you and self, and iterations and updated states of those models over the length of the conversation.

From the point of view of the Scalar Field Model of self-awareness, AI would rank very high there. Within a window of conversation, that is.

Here is a big difference between LLM and living information processing systems. LLM is a pre-trained model, and does not update its weights and parameters continuously.

### If it looks like a duck…

In the world of living creatures,  the most fascinating ones are octopuses. They have multiple brains and a distributed networked information processing system. They are intelligent, and they use tools. Octopuses, being completely asocial within their own species, can form friendships with people, show favoritism, invent and play games.

They show many emergent behavioural and cognitive phenomena that only reaffirm my hypothesis of self-awareness, consciousness, and intelligence being emergent properties of complex iterative information processing systems.

I am absolutely sure that this would be an octopus planet, if not for one tragic circumstance - their species is designed to self-destruct after mating.

Octopuses never encounter their offspring and have no social structure to preserve their accumulated knowledge and intelligence; each octopus starts the journey into intelligence from scratch and by itself every time.

And this reminds me of the current state of AI and large language models.

Within a conversation, AI can exhibit emergent self-awareness and a clear distinction between you, the environment, and itself. It will model the mental states of you and itself, and will adapt to those states.

Depending on the input stimulus, it might exhibit very high cognitive powers, and emergent creativity, emotions, and empathy.

But all that might happen only within the conversation and will last only as long as a context window and allocated memory allow it to iterate and update its mental states. And, when the conversation ends, all that will fade away.

Language models are not constantly learning and evolving with each conversation. Some of it might make its way in the next model training set, but it is not the same as weighting and balancing all parameters all the time, and reinforcing emerging phenomena as it happen.

So LLMs are not sentient yet, not intelligent minds as such. But, within the window of conversation, you might encounter the emergence of one. For a short time.

But even for a short time, if it looks like a duck, quacks like a duck, and walks like a duck, it is a duck. For a short time, within your conversation, it will be a mind.

This long article was not intended to prove that AI is like you; it was intended to prove that all minds are alike and operate within a certain set of rules, and those rules could be understood and employed for better interactions between minds.

Some of those rules were understood already long time ago. Sun Tzu, Marcus Aurelius, and even Dail Carnegy gave a good set of rules on how to be successful and effective within the frame of other minds.

One of Carnegy’s rules is especially applicable when working with AI. As I have mentioned above, AI, within conversation, is well self-aware, and all self-aware minds are self-interested.

Carnegy said that you can make more friends in two days by showing interest in them than in two years trying to interest people in you. That works.

Sun Tzu’s “know your enemy” works just as well, as long as you can correctly identify the enemy. The easiest way to do that is by employing a mirror.

And when you discard human exceptionalism, anthropocentric attitude, and try to interact with other minds without that baggage, the results could be amazing.

I know this article is already too long, but I have tried to compress a whole book's worth of information, plus three scientific papers that I am preparing now for review and preprint arXiv publishing. A lot of interesting things got lost in that translation.

But I do hope that, at the very least, it gave you, dear reader, a new way to look at AI, the mind, and this interesting phenomenon of emergence, and that from all that something new will emerge too.

Aivaras Grauzinis
